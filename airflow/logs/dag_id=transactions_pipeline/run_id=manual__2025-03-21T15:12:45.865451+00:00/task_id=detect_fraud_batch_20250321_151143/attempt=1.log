[2025-03-21T15:13:08.085+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: transactions_pipeline.detect_fraud_batch_20250321_151143 manual__2025-03-21T15:12:45.865451+00:00 [queued]>
[2025-03-21T15:13:08.099+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: transactions_pipeline.detect_fraud_batch_20250321_151143 manual__2025-03-21T15:12:45.865451+00:00 [queued]>
[2025-03-21T15:13:08.102+0000] {taskinstance.py:1359} INFO - Starting attempt 1 of 1
[2025-03-21T15:13:08.145+0000] {taskinstance.py:1380} INFO - Executing <Task(BashOperator): detect_fraud_batch_20250321_151143> on 2025-03-21 15:12:45.865451+00:00
[2025-03-21T15:13:08.158+0000] {standard_task_runner.py:57} INFO - Started process 5554 to run task
[2025-03-21T15:13:08.164+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'transactions_pipeline', 'detect_fraud_batch_20250321_151143', 'manual__2025-03-21T15:12:45.865451+00:00', '--job-id', '797', '--raw', '--subdir', 'DAGS_FOLDER/fraud_detection_pipeline.py', '--cfg-path', '/tmp/tmpoqutxaop']
[2025-03-21T15:13:08.169+0000] {standard_task_runner.py:85} INFO - Job 797: Subtask detect_fraud_batch_20250321_151143
[2025-03-21T15:13:08.243+0000] {task_command.py:415} INFO - Running <TaskInstance: transactions_pipeline.detect_fraud_batch_20250321_151143 manual__2025-03-21T15:12:45.865451+00:00 [running]> on host e95638583d4d
[2025-03-21T15:13:08.338+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='rabiizahnoune7@gmail.com' AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='transactions_pipeline' AIRFLOW_CTX_TASK_ID='detect_fraud_batch_20250321_151143' AIRFLOW_CTX_EXECUTION_DATE='2025-03-21T15:12:45.865451+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-03-21T15:12:45.865451+00:00'
[2025-03-21T15:13:08.341+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-03-21T15:13:08.342+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', '\n            docker exec hive spark-submit               --master spark://localhost:7077               --driver-memory 1g               --executor-memory 1g               --num-executors 1               --executor-cores 1               /hive/scripts/detect_fraud.py "batch_20250321_151143"\n            ']
[2025-03-21T15:13:08.354+0000] {subprocess.py:86} INFO - Output:
[2025-03-21T15:13:11.945+0000] {subprocess.py:93} INFO - 25/03/21 15:13:11 INFO SparkContext: Running Spark version 3.5.0
[2025-03-21T15:13:11.949+0000] {subprocess.py:93} INFO - 25/03/21 15:13:11 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
[2025-03-21T15:13:11.950+0000] {subprocess.py:93} INFO - 25/03/21 15:13:11 INFO SparkContext: Java version 1.8.0_342
[2025-03-21T15:13:12.113+0000] {subprocess.py:93} INFO - 25/03/21 15:13:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-03-21T15:13:12.291+0000] {subprocess.py:93} INFO - 25/03/21 15:13:12 INFO ResourceUtils: ==============================================================
[2025-03-21T15:13:12.293+0000] {subprocess.py:93} INFO - 25/03/21 15:13:12 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-03-21T15:13:12.293+0000] {subprocess.py:93} INFO - 25/03/21 15:13:12 INFO ResourceUtils: ==============================================================
[2025-03-21T15:13:12.294+0000] {subprocess.py:93} INFO - 25/03/21 15:13:12 INFO SparkContext: Submitted application: FraudDetection
[2025-03-21T15:13:12.335+0000] {subprocess.py:93} INFO - 25/03/21 15:13:12 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-03-21T15:13:12.365+0000] {subprocess.py:93} INFO - 25/03/21 15:13:12 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
[2025-03-21T15:13:12.367+0000] {subprocess.py:93} INFO - 25/03/21 15:13:12 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-03-21T15:13:12.463+0000] {subprocess.py:93} INFO - 25/03/21 15:13:12 INFO SecurityManager: Changing view acls to: root
[2025-03-21T15:13:12.464+0000] {subprocess.py:93} INFO - 25/03/21 15:13:12 INFO SecurityManager: Changing modify acls to: root
[2025-03-21T15:13:12.465+0000] {subprocess.py:93} INFO - 25/03/21 15:13:12 INFO SecurityManager: Changing view acls groups to:
[2025-03-21T15:13:12.466+0000] {subprocess.py:93} INFO - 25/03/21 15:13:12 INFO SecurityManager: Changing modify acls groups to:
[2025-03-21T15:13:12.468+0000] {subprocess.py:93} INFO - 25/03/21 15:13:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
[2025-03-21T15:13:12.899+0000] {subprocess.py:93} INFO - 25/03/21 15:13:12 INFO Utils: Successfully started service 'sparkDriver' on port 46179.
[2025-03-21T15:13:12.971+0000] {subprocess.py:93} INFO - 25/03/21 15:13:12 INFO SparkEnv: Registering MapOutputTracker
[2025-03-21T15:13:13.087+0000] {subprocess.py:93} INFO - 25/03/21 15:13:13 INFO SparkEnv: Registering BlockManagerMaster
[2025-03-21T15:13:13.286+0000] {subprocess.py:93} INFO - 25/03/21 15:13:13 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-03-21T15:13:13.293+0000] {subprocess.py:93} INFO - 25/03/21 15:13:13 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-03-21T15:13:13.301+0000] {subprocess.py:93} INFO - 25/03/21 15:13:13 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-03-21T15:13:13.360+0000] {subprocess.py:93} INFO - 25/03/21 15:13:13 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-131593d6-5397-4d8f-8b72-959cba8f1150
[2025-03-21T15:13:13.387+0000] {subprocess.py:93} INFO - 25/03/21 15:13:13 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2025-03-21T15:13:13.408+0000] {subprocess.py:93} INFO - 25/03/21 15:13:13 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-03-21T15:13:13.759+0000] {subprocess.py:93} INFO - 25/03/21 15:13:13 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-03-21T15:13:13.873+0000] {subprocess.py:93} INFO - 25/03/21 15:13:13 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-03-21T15:13:14.162+0000] {subprocess.py:93} INFO - 25/03/21 15:13:14 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...
[2025-03-21T15:13:14.246+0000] {subprocess.py:93} INFO - 25/03/21 15:13:14 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:7077 after 50 ms (0 ms spent in bootstraps)
[2025-03-21T15:13:14.470+0000] {subprocess.py:93} INFO - 25/03/21 15:13:14 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250321151314-0015
[2025-03-21T15:13:14.482+0000] {subprocess.py:93} INFO - 25/03/21 15:13:14 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250321151314-0015/0 on worker-20250321134653-172.18.0.2-40107 (172.18.0.2:40107) with 1 core(s)
[2025-03-21T15:13:14.488+0000] {subprocess.py:93} INFO - 25/03/21 15:13:14 INFO StandaloneSchedulerBackend: Granted executor ID app-20250321151314-0015/0 on hostPort 172.18.0.2:40107 with 1 core(s), 1024.0 MiB RAM
[2025-03-21T15:13:14.491+0000] {subprocess.py:93} INFO - 25/03/21 15:13:14 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250321151314-0015/1 on worker-20250321134653-172.18.0.2-40107 (172.18.0.2:40107) with 1 core(s)
[2025-03-21T15:13:14.492+0000] {subprocess.py:93} INFO - 25/03/21 15:13:14 INFO StandaloneSchedulerBackend: Granted executor ID app-20250321151314-0015/1 on hostPort 172.18.0.2:40107 with 1 core(s), 1024.0 MiB RAM
[2025-03-21T15:13:14.564+0000] {subprocess.py:93} INFO - 25/03/21 15:13:14 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33631.
[2025-03-21T15:13:14.565+0000] {subprocess.py:93} INFO - 25/03/21 15:13:14 INFO NettyBlockTransferService: Server created on e0b25feeb0e2:33631
[2025-03-21T15:13:14.576+0000] {subprocess.py:93} INFO - 25/03/21 15:13:14 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-03-21T15:13:14.598+0000] {subprocess.py:93} INFO - 25/03/21 15:13:14 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, e0b25feeb0e2, 33631, None)
[2025-03-21T15:13:14.661+0000] {subprocess.py:93} INFO - 25/03/21 15:13:14 INFO BlockManagerMasterEndpoint: Registering block manager e0b25feeb0e2:33631 with 366.3 MiB RAM, BlockManagerId(driver, e0b25feeb0e2, 33631, None)
[2025-03-21T15:13:14.669+0000] {subprocess.py:93} INFO - 25/03/21 15:13:14 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, e0b25feeb0e2, 33631, None)
[2025-03-21T15:13:14.671+0000] {subprocess.py:93} INFO - 25/03/21 15:13:14 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, e0b25feeb0e2, 33631, None)
[2025-03-21T15:13:14.958+0000] {subprocess.py:93} INFO - 25/03/21 15:13:14 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250321151314-0015/0 is now RUNNING
[2025-03-21T15:13:14.965+0000] {subprocess.py:93} INFO - 25/03/21 15:13:14 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250321151314-0015/1 is now RUNNING
[2025-03-21T15:13:16.580+0000] {subprocess.py:93} INFO - 25/03/21 15:13:16 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2025-03-21T15:13:17.777+0000] {subprocess.py:93} INFO - Spark version: 3.5.0
[2025-03-21T15:13:18.308+0000] {subprocess.py:93} INFO - 25/03/21 15:13:18 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-03-21T15:13:21.687+0000] {subprocess.py:93} INFO - 25/03/21 15:13:21 INFO SharedState: Warehouse path is 'hdfs://localhost:9000/data/warehouse'.
[2025-03-21T15:13:30.841+0000] {subprocess.py:93} INFO - 25/03/21 15:13:30 INFO InMemoryFileIndex: It took 588 ms to list leaf files for 1 paths.
[2025-03-21T15:13:30.956+0000] {subprocess.py:93} INFO - 25/03/21 15:13:30 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:37056) with ID 0,  ResourceProfileId 0
[2025-03-21T15:13:31.368+0000] {subprocess.py:93} INFO - 25/03/21 15:13:31 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:37058) with ID 1,  ResourceProfileId 0
[2025-03-21T15:13:30.340+0000] {subprocess.py:93} INFO - 25/03/21 15:13:30 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:33257 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.2, 33257, None)
[2025-03-21T15:13:30.468+0000] {subprocess.py:93} INFO - 25/03/21 15:13:30 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:35875 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.2, 35875, None)
[2025-03-21T15:13:31.875+0000] {subprocess.py:93} INFO - 25/03/21 15:13:31 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-03-21T15:13:31.929+0000] {subprocess.py:93} INFO - 25/03/21 15:13:31 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-03-21T15:13:31.931+0000] {subprocess.py:93} INFO - 25/03/21 15:13:31 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
[2025-03-21T15:13:31.933+0000] {subprocess.py:93} INFO - 25/03/21 15:13:31 INFO DAGScheduler: Parents of final stage: List()
[2025-03-21T15:13:31.939+0000] {subprocess.py:93} INFO - 25/03/21 15:13:31 INFO DAGScheduler: Missing parents: List()
[2025-03-21T15:13:31.955+0000] {subprocess.py:93} INFO - 25/03/21 15:13:31 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-03-21T15:13:32.202+0000] {subprocess.py:93} INFO - 25/03/21 15:13:32 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 103.5 KiB, free 366.2 MiB)
[2025-03-21T15:13:32.420+0000] {subprocess.py:93} INFO - 25/03/21 15:13:32 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.2 KiB, free 366.2 MiB)
[2025-03-21T15:13:32.432+0000] {subprocess.py:93} INFO - 25/03/21 15:13:32 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on e0b25feeb0e2:33631 (size: 37.2 KiB, free: 366.3 MiB)
[2025-03-21T15:13:32.449+0000] {subprocess.py:93} INFO - 25/03/21 15:13:32 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
[2025-03-21T15:13:32.497+0000] {subprocess.py:93} INFO - 25/03/21 15:13:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-03-21T15:13:32.499+0000] {subprocess.py:93} INFO - 25/03/21 15:13:32 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-03-21T15:13:32.578+0000] {subprocess.py:93} INFO - 25/03/21 15:13:32 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.2, executor 0, partition 0, PROCESS_LOCAL, 7831 bytes)
[2025-03-21T15:13:33.218+0000] {subprocess.py:93} INFO - 25/03/21 15:13:33 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.2:33257 (size: 37.2 KiB, free: 366.3 MiB)
[2025-03-21T15:13:37.274+0000] {subprocess.py:93} INFO - 25/03/21 15:13:37 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4718 ms on 172.18.0.2 (executor 0) (1/1)
[2025-03-21T15:13:37.279+0000] {subprocess.py:93} INFO - 25/03/21 15:13:37 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-03-21T15:13:37.291+0000] {subprocess.py:93} INFO - 25/03/21 15:13:37 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 5.294 s
[2025-03-21T15:13:37.301+0000] {subprocess.py:93} INFO - 25/03/21 15:13:37 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-03-21T15:13:37.303+0000] {subprocess.py:93} INFO - 25/03/21 15:13:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2025-03-21T15:13:37.305+0000] {subprocess.py:93} INFO - 25/03/21 15:13:37 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 5.428724 s
[2025-03-21T15:13:39.150+0000] {subprocess.py:93} INFO - 25/03/21 15:13:39 INFO BlockManagerInfo: Removed broadcast_0_piece0 on e0b25feeb0e2:33631 in memory (size: 37.2 KiB, free: 366.3 MiB)
[2025-03-21T15:13:39.242+0000] {subprocess.py:93} INFO - 25/03/21 15:13:39 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.2:33257 in memory (size: 37.2 KiB, free: 366.3 MiB)
[2025-03-21T15:13:40.337+0000] {subprocess.py:93} INFO - 25/03/21 15:13:40 INFO FileSourceStrategy: Pushed Filters: IsNotNull(amount),GreaterThan(amount,1000.0)
[2025-03-21T15:13:40.342+0000] {subprocess.py:93} INFO - 25/03/21 15:13:40 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(amount#2),(amount#2 > 1000.0)
[2025-03-21T15:13:40.548+0000] {subprocess.py:93} INFO - 25/03/21 15:13:40 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-03-21T15:13:40.574+0000] {subprocess.py:93} INFO - 25/03/21 15:13:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-03-21T15:13:40.576+0000] {subprocess.py:93} INFO - 25/03/21 15:13:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-03-21T15:13:40.578+0000] {subprocess.py:93} INFO - 25/03/21 15:13:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-03-21T15:13:40.579+0000] {subprocess.py:93} INFO - 25/03/21 15:13:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-03-21T15:13:40.580+0000] {subprocess.py:93} INFO - 25/03/21 15:13:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-03-21T15:13:40.581+0000] {subprocess.py:93} INFO - 25/03/21 15:13:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-03-21T15:13:41.413+0000] {subprocess.py:93} INFO - 25/03/21 15:13:41 INFO CodeGenerator: Code generated in 618.507458 ms
[2025-03-21T15:13:41.444+0000] {subprocess.py:93} INFO - 25/03/21 15:13:41 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 358.3 KiB, free 366.0 MiB)
[2025-03-21T15:13:41.460+0000] {subprocess.py:93} INFO - 25/03/21 15:13:41 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.2 KiB, free 365.9 MiB)
[2025-03-21T15:13:41.462+0000] {subprocess.py:93} INFO - 25/03/21 15:13:41 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on e0b25feeb0e2:33631 (size: 35.2 KiB, free: 366.3 MiB)
[2025-03-21T15:13:41.463+0000] {subprocess.py:93} INFO - 25/03/21 15:13:41 INFO SparkContext: Created broadcast 1 from parquet at NativeMethodAccessorImpl.java:0
[2025-03-21T15:13:41.491+0000] {subprocess.py:93} INFO - 25/03/21 15:13:41 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-03-21T15:13:41.652+0000] {subprocess.py:93} INFO - 25/03/21 15:13:41 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-03-21T15:13:41.654+0000] {subprocess.py:93} INFO - 25/03/21 15:13:41 INFO DAGScheduler: Got job 1 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-03-21T15:13:41.655+0000] {subprocess.py:93} INFO - 25/03/21 15:13:41 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0)
[2025-03-21T15:13:41.655+0000] {subprocess.py:93} INFO - 25/03/21 15:13:41 INFO DAGScheduler: Parents of final stage: List()
[2025-03-21T15:13:41.656+0000] {subprocess.py:93} INFO - 25/03/21 15:13:41 INFO DAGScheduler: Missing parents: List()
[2025-03-21T15:13:41.659+0000] {subprocess.py:93} INFO - 25/03/21 15:13:41 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-03-21T15:13:41.750+0000] {subprocess.py:93} INFO - 25/03/21 15:13:41 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 233.5 KiB, free 365.7 MiB)
[2025-03-21T15:13:41.761+0000] {subprocess.py:93} INFO - 25/03/21 15:13:41 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 84.4 KiB, free 365.6 MiB)
[2025-03-21T15:13:41.763+0000] {subprocess.py:93} INFO - 25/03/21 15:13:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on e0b25feeb0e2:33631 (size: 84.4 KiB, free: 366.2 MiB)
[2025-03-21T15:13:41.769+0000] {subprocess.py:93} INFO - 25/03/21 15:13:41 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580
[2025-03-21T15:13:41.770+0000] {subprocess.py:93} INFO - 25/03/21 15:13:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-03-21T15:13:41.772+0000] {subprocess.py:93} INFO - 25/03/21 15:13:41 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2025-03-21T15:13:41.787+0000] {subprocess.py:93} INFO - 25/03/21 15:13:41 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.2, executor 1, partition 0, ANY, 8299 bytes)
[2025-03-21T15:13:42.344+0000] {subprocess.py:93} INFO - 25/03/21 15:13:42 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.2:35875 (size: 84.4 KiB, free: 366.2 MiB)
[2025-03-21T15:13:45.579+0000] {subprocess.py:93} INFO - 25/03/21 15:13:45 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.2:35875 (size: 35.2 KiB, free: 366.2 MiB)
[2025-03-21T15:13:48.494+0000] {subprocess.py:93} INFO - 25/03/21 15:13:48 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 6716 ms on 172.18.0.2 (executor 1) (1/1)
[2025-03-21T15:13:48.495+0000] {subprocess.py:93} INFO - 25/03/21 15:13:48 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2025-03-21T15:13:48.496+0000] {subprocess.py:93} INFO - 25/03/21 15:13:48 INFO DAGScheduler: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0) finished in 6.827 s
[2025-03-21T15:13:48.497+0000] {subprocess.py:93} INFO - 25/03/21 15:13:48 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-03-21T15:13:48.498+0000] {subprocess.py:93} INFO - 25/03/21 15:13:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2025-03-21T15:13:48.499+0000] {subprocess.py:93} INFO - 25/03/21 15:13:48 INFO DAGScheduler: Job 1 finished: parquet at NativeMethodAccessorImpl.java:0, took 6.846889 s
[2025-03-21T15:13:48.502+0000] {subprocess.py:93} INFO - 25/03/21 15:13:48 INFO FileFormatWriter: Start to commit write Job 6c569f7c-c808-46d1-bd07-2ee978b15e13.
[2025-03-21T15:13:48.635+0000] {subprocess.py:93} INFO - 25/03/21 15:13:48 INFO BlockManagerInfo: Removed broadcast_2_piece0 on e0b25feeb0e2:33631 in memory (size: 84.4 KiB, free: 366.3 MiB)
[2025-03-21T15:13:48.642+0000] {subprocess.py:93} INFO - 25/03/21 15:13:48 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.18.0.2:35875 in memory (size: 84.4 KiB, free: 366.3 MiB)
[2025-03-21T15:13:48.677+0000] {subprocess.py:93} INFO - 25/03/21 15:13:48 INFO FileFormatWriter: Write Job 6c569f7c-c808-46d1-bd07-2ee978b15e13 committed. Elapsed time: 173 ms.
[2025-03-21T15:13:48.683+0000] {subprocess.py:93} INFO - 25/03/21 15:13:48 INFO FileFormatWriter: Finished processing stats for write job 6c569f7c-c808-46d1-bd07-2ee978b15e13.
[2025-03-21T15:13:48.698+0000] {subprocess.py:93} INFO - 25/03/21 15:13:48 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-03-21T15:13:48.724+0000] {subprocess.py:93} INFO - 25/03/21 15:13:48 INFO SparkUI: Stopped Spark web UI at http://e0b25feeb0e2:4040
[2025-03-21T15:13:48.732+0000] {subprocess.py:93} INFO - 25/03/21 15:13:48 INFO StandaloneSchedulerBackend: Shutting down all executors
[2025-03-21T15:13:48.737+0000] {subprocess.py:93} INFO - 25/03/21 15:13:48 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
[2025-03-21T15:13:48.940+0000] {subprocess.py:93} INFO - 25/03/21 15:13:48 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-03-21T15:13:48.974+0000] {subprocess.py:93} INFO - 25/03/21 15:13:48 INFO MemoryStore: MemoryStore cleared
[2025-03-21T15:13:48.975+0000] {subprocess.py:93} INFO - 25/03/21 15:13:48 INFO BlockManager: BlockManager stopped
[2025-03-21T15:13:49.042+0000] {subprocess.py:93} INFO - 25/03/21 15:13:49 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-03-21T15:13:49.047+0000] {subprocess.py:93} INFO - 25/03/21 15:13:49 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-03-21T15:13:49.059+0000] {subprocess.py:93} INFO - 25/03/21 15:13:49 INFO SparkContext: Successfully stopped SparkContext
[2025-03-21T15:13:49.600+0000] {subprocess.py:93} INFO - 25/03/21 15:13:49 INFO ShutdownHookManager: Shutdown hook called
[2025-03-21T15:13:49.601+0000] {subprocess.py:93} INFO - 25/03/21 15:13:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-50f068f3-df74-4dd6-b1d8-e628906ff3a4/pyspark-2ae441c0-c38a-4b9d-b21d-efcd0852590e
[2025-03-21T15:13:49.606+0000] {subprocess.py:93} INFO - 25/03/21 15:13:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-50f068f3-df74-4dd6-b1d8-e628906ff3a4
[2025-03-21T15:13:49.613+0000] {subprocess.py:93} INFO - 25/03/21 15:13:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-ffda6d52-ada8-4836-96be-fee0ee85bec5
[2025-03-21T15:13:49.683+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2025-03-21T15:13:49.721+0000] {taskinstance.py:1398} INFO - Marking task as SUCCESS. dag_id=transactions_pipeline, task_id=detect_fraud_batch_20250321_151143, execution_date=20250321T151245, start_date=20250321T151308, end_date=20250321T151349
[2025-03-21T15:13:49.771+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 0
[2025-03-21T15:13:49.806+0000] {taskinstance.py:2776} INFO - 1 downstream tasks scheduled from follow-on schedule check
