

services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres_db_volume:/var/lib/postgresql/data
      - ./init-sql:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    networks:
      - airflow_network

  airflow-init:
    image: apache/airflow:2.7.1
    environment:
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - /var/run/docker.sock:/var/run/docker.sock
      - hadoop_datanode:/mnt/hadoop_data
      - transactions_data:/data/transactions  # Volume partagé
    entrypoint: bash -c "airflow db init && airflow users create --username admin --password admin --firstname Admin --lastname User --email admin@example.com --role Admin"
    depends_on:
      - postgres
    networks:
      - airflow_network

  airflow-webserver:
    build:
      context: ./
      dockerfile: apache.Dockerfile
    restart: always
    environment:
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
    ports:
      - "8080:8080"
    depends_on:
      - postgres
      - airflow-init
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - /var/run/docker.sock:/var/run/docker.sock
      - hadoop_datanode:/mnt/hadoop_data
      - ./hive/scripts:/scripts
      - transactions_data:/data/transactions  # Volume partagé
    command: webserver
    networks:
      - airflow_network

  airflow-scheduler:
    build:
      context: ./
      dockerfile: apache.Dockerfile
    restart: always
    environment:
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - hadoop_datanode:/mnt/hadoop_data
      - /var/run/docker.sock:/var/run/docker.sock
      - ./hive/scripts:/scripts
      - transactions_data:/data/transactions  # Volume partagé
    command: scheduler
    depends_on:
      - postgres
      - airflow-init
    networks:
      - airflow_network


  hive:
    build:
      context: ./hive
      dockerfile: Dockerfile
    container_name: hive
    ports:
      - "10000:10000"  # HiveServer2
      - "10002:10002"  # Hive Metastore Thrift
      - "9870:9870"    # NameNode Web UI
      - "9000:9000"    # HDFS NameNode (port de données)
    volumes:
      - transactions_data:/data/transactions  # Volume partagé
      - ./data/namenode:/data/namenode        # Pour le NameNode
      - ./data/datanode:/data/datanode        # Pour le DataNode
      - ./data/warehouse:/data/warehouse      # Pour le warehouse Hive
    networks:
      - airflow_network

  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - airflow_network

  kafka:
    build:
      context: ./kafka
      dockerfile: Dockerfile
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
    volumes:
      - ./kafka:/application
      - /var/run/docker.sock:/var/run/docker.sock  # Ajout du socket Docker
      - transactions_data:/data/transactions  # Volume nommé partagé
    depends_on:
      - zookeeper
    networks:
      - airflow_network

  api-transactions:
    build:
      context: ./api/transanctions
      dockerfile: Dockerfile
    ports:
      - "5000:5000"
    environment:
      - KAFKA_BROKER=kafka:9092
      - KAFKA_TOPIC=transactions_topic
    depends_on:
      - kafka
    networks:
      - airflow_network

  api-customers:
    build:
      context: ./api/customers
      dockerfile: Dockerfile
    ports:
      - "5001:5001"
    environment:
      - KAFKA_BROKER=kafka:9092
      - KAFKA_TOPIC=customers_topic
    depends_on:
      - kafka
    networks:
      - airflow_network

  api-externaldata:
    build:
      context: ./api/external  # Ajusté pour correspondre au nom du service
      dockerfile: Dockerfile
    ports:
      - "5002:5002"
    environment:
      - KAFKA_BROKER=kafka:9092
      - KAFKA_TOPIC=external_data_topic
    depends_on:
      - kafka
    networks:
      - airflow_network

volumes:
  postgres_db_volume:
  namenode_data:
  datanode1_data:
  hadoop_datanode:
  hive-data:
  transactions_data:

networks:
  airflow_network:
    driver: bridge