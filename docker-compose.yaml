

services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres_db_volume:/var/lib/postgresql/data
      - ./init-sql:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    networks:
      - airflow_network

  airflow-init:
    image: apache/airflow:2.7.1
    environment:
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW__SMTP__SMTP_USER=rabiizahnoune7@gmail.com
      - AIRFLOW__SMTP__SMTP_PASSWORD=xcci gubd zyoi wvjq
      - AIRFLOW__SMTP__SMTP_HOST=smtp.gmail.com
      - AIRFLOW__SMTP__SMTP_STARTTLS=True
      - AIRFLOW__SMTP__SMTP_SSL=False
      - AIRFLOW__SMTP__SMTP_PORT=587
      - AIRFLOW__SMTP__SMTP_MAIL_FROM=rabiizahnoune7@gmail.com
      - AIRFLOW__SMTP__SMTP_TIMEOUT=30
      - AIRFLOW__SMTP__SMTP_RETRY_LIMIT=5
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - /var/run/docker.sock:/var/run/docker.sock
      - hadoop_datanode:/mnt/hadoop_data
      - transactions_data:/data/transactions  # Volume partagé
    entrypoint: bash -c "airflow db init && airflow users create --username admin --password admin --firstname Admin --lastname User --email admin@example.com --role Admin"
    depends_on:
      - postgres
    networks:
      - airflow_network

  airflow-webserver:
    build:
      context: ./
      dockerfile: apache.Dockerfile
    restart: always
    environment:
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW__SMTP__SMTP_USER=rabiizahnoune7@gmail.com
      - AIRFLOW__SMTP__SMTP_PASSWORD=xcci gubd zyoi wvjq
      - AIRFLOW__SMTP__SMTP_HOST=smtp.gmail.com
      - AIRFLOW__SMTP__SMTP_STARTTLS=True
      - AIRFLOW__SMTP__SMTP_SSL=False
      - AIRFLOW__SMTP__SMTP_PORT=587
      - AIRFLOW__SMTP__SMTP_MAIL_FROM=rabiizahnoune7@gmail.com
      - AIRFLOW__SMTP__SMTP_TIMEOUT=30
      - AIRFLOW__SMTP__SMTP_RETRY_LIMIT=5
    ports:
      - "8080:8080"
    depends_on:
      - postgres
      - airflow-init
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - /var/run/docker.sock:/var/run/docker.sock
      - hadoop_datanode:/mnt/hadoop_data
      - ./hive/scripts:/scripts
      - transactions_data:/data/transactions  # Volume partagé
    command: webserver
    networks:
      - airflow_network

  airflow-scheduler:
    build:
      context: ./
      dockerfile: apache.Dockerfile
    restart: always
    environment:
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW__SMTP__SMTP_USER=rabiizahnoune7@gmail.com
      - AIRFLOW__SMTP__SMTP_PASSWORD=xcci gubd zyoi wvjq
      - AIRFLOW__SMTP__SMTP_HOST=smtp.gmail.com
      - AIRFLOW__SMTP__SMTP_STARTTLS=True
      - AIRFLOW__SMTP__SMTP_SSL=False
      - AIRFLOW__SMTP__SMTP_PORT=587
      - AIRFLOW__SMTP__SMTP_MAIL_FROM=rabiizahnoune7@gmail.com
      - AIRFLOW__SMTP__SMTP_TIMEOUT=30
      - AIRFLOW__SMTP__SMTP_RETRY_LIMIT=5
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - hadoop_datanode:/mnt/hadoop_data
      - /var/run/docker.sock:/var/run/docker.sock
      - ./hive/scripts:/scripts
      - transactions_data:/data/transactions  # Volume partagé
    command: scheduler
    depends_on:
      - postgres
      - airflow-init
    networks:
      - airflow_network


  hive:
    build:
      context: ./hive
      dockerfile: Dockerfile
    container_name: hive
    ports:
      - "10000:10000"
      - "10002:10002"
      - "9870:9870"
      - "9000:9000"
      - "7077:7077"
    volumes:
      - transactions_data:/data/transactions
      - ./data/namenode:/data/namenode
      - ./data/datanode:/data/datanode
      - ./data/warehouse:/data/warehouse
      - ./hive/scripts:/hive/scripts
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: "4G"
    networks:
      - airflow_network
  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - airflow_network

  kafka:
    build:
      context: ./kafka
      dockerfile: Dockerfile
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
    volumes:
      - ./kafka:/application
      - /var/run/docker.sock:/var/run/docker.sock  # Ajout du socket Docker
      - transactions_data:/data/transactions  # Volume nommé partagé
    depends_on:
      - zookeeper
    networks:
      - airflow_network

  api-transactions:
    build:
      context: ./api/transanctions
      dockerfile: Dockerfile
    ports:
      - "5000:5000"
    environment:
      - KAFKA_BROKER=kafka:9092
      - KAFKA_TOPIC=transactions_topic
    depends_on:
      - kafka
    networks:
      - airflow_network

  api-customers:
    build:
      context: ./api/customers
      dockerfile: Dockerfile
    ports:
      - "5001:5001"
    environment:
      - KAFKA_BROKER=kafka:9092
      - KAFKA_TOPIC=customers_topic
    depends_on:
      - kafka
    networks:
      - airflow_network

  api-externaldata:
    build:
      context: ./api/external  # Ajusté pour correspondre au nom du service
      dockerfile: Dockerfile
    ports:
      - "5002:5002"
    environment:
      - KAFKA_BROKER=kafka:9092
      - KAFKA_TOPIC=external_data_topic
    depends_on:
      - kafka
    networks:
      - airflow_network

volumes:
  postgres_db_volume:
  namenode_data:
  datanode1_data:
  hadoop_datanode:
  hive-data:
  transactions_data:

networks:
  airflow_network:
    driver: bridge


# INSERT INTO TABLE fraud_detections
# SELECT
#     t.transaction_id,
#     t.date_time,
#     t.amount,
#     t.customer_id,
#     t.location,
#     'High Amount' AS fraud_reason
# FROM transactions t
# WHERE t.batch_id = 'batch_20250320_112938'
# AND t.amount > 1000;